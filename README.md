# Real_vs_deepfake_cnn
Differentiating Between Real and Artificially Generated Images with Fully-Connected Networks and Convolutional Neural Networks





In this work, I investigate the viability of using both conventional fully connected backpropagation network models and convolutional neural network models for differentiating between artificially generated images and real images. My main finding is that both of these types of networks can be viable solutions to this problem, although the accuracy and efficiency of this method is greatly variable based on a multitude of factors. Standard fully-connected networks appear to return slightly less accurate classifications in a much lesser time than the more advanced CNNs, which are more accurate, but more computationally expensive and time-intensive to train. By increasing the depth of the network and the number of epochs, and performing techniques to combat overfitting, it is possible to use both of these types of artificial neural networks to differentiate between real and artificially generated images.


In this research experiment, I used 3000 1024x1024 pixels images of human faces, of which 1500 were of real humans, and 1500 were artificially generated by NVIDIA’s StyleGAN generative adversarial network. Every photo was pulled from the aforementioned “whichfaceisreal.com” website, which presents one real image drawn from the “Flickr-Faces-HQ Dataset” (FFHQ), which consists of headshots of real people pulled from the image-sharing website Flickr, and one headshot artificially generated by StyleGAN.2 A simple Python script was used to repeatedly download the two images presented on the page, and refresh, until 3000 images were collected. All fake images were stored by “whichfaceisreal.com” with names such as “image-2019-02-18_231046.jpeg”, starting with the word “image”, and containing a date, and all real images were stored by the website with names such as “25209.jpeg” (only a number), so it was possible to segregate the images based on class immediately upon downloading. The images were then split randomly 80:20 into training and test data, leaving 2400 images (1200 real and 1200 artificial) for the training dataset and 600 images (300 real and 300 artificial) for the testing dataset. All images were then downsized to 200x200 pixels from 1024x1024 pixels, as the 1024x1024 pixel images took unrealistically long to train locally on my PC. 

The hypothesis that CNNs would be able to more accurately learn to discriminate between real images of people and ones artificially generated by a GAN is supported by these results. Between all of the CNN models trained, they averaged an accuracy of 74.47% in identifying whether images were artificially generated or not. This compares favorably to the average 63.50% accuracy on the same task for fully-connected backpropagation networks. While it should be kept in mind that not all of the CNNs were analogous to a specific FC in this research, an overall difference of over 11% in accuracy is not trivial and suggests that CNNs might be better suited to this task if accuracy is crucial and computing resources are plenty. 
These results are quite impressive for three reasons, all stemming from hardware limitations: 1) limited amount of training images (2400), 2) decreased size of training images (1024x1024 pixels to 200x200 pixels), 3) decreased depth of networks. If state-of-the-art deep learning hardware could be used to process more higher quality images with deeper networks, it is very probable that much more accurate results could be had. I suspect that some of the visual artifacts that might distinguish between real and artificially generated images are removed when the size of the photo is decreased so drastically, so even rectifying issue 2) alone with the right hardware might greatly improve the results of these networks.
